# ML-Fundamentals
- [ConvNetJS, DeepLearning in Your Browser](https://cs.stanford.edu/people/karpathy/convnetjs/)
- [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples)
- [Code for Hands-on ML using Scikit-Learn, Tensorflow, and Keras](https://github.com/ageron/handson-ml3)
- [TensorFlow Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)
- [Code for ML with PyTorch and Sci-kit Learn](https://github.com/rasbt/machine-learning-book)
- [Large Model Parallelism Notebook](https://github.com/hundredblocks/large-model-parallelism)
- [Datascience IPython Notebooks](https://github.com/donnemartin/data-science-ipython-notebooks)
- [Code for Artificial Intelligence, A Modern Approach](https://github.com/aimacode/aima-python)
- [Christopher Olah's Blog](https://christopherolah.wordpress.com)
- [Code for NNs and DeepLearning book](http://neuralnetworksanddeeplearning.com)
- [ML Fundamentals lectures](https://www.youtube.com/watch?v=yDLKJtOVx5c)
- [The Unreasonable Effectiveness of RNNs, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Minimal and Clean RL examples](https://github.com/rlcode/reinforcement-learning)
- [Python NumPy Tutorial, Stanford](https://cs231n.github.io/python-numpy-tutorial/)
- [Which GPUs to get for Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)
- [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
- [Deep Learning Tuning Playbook, Google Research](https://github.com/google-research/tuning_playbook)
- [Micrograd, Andrej karpathy](https://github.com/karpathy/micrograd) fav <3
- [Automatic Differentiation, Mark Saroufim](https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6)
- [Machine Learning Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)
- [What is Softmax? Sebastian Raschka](https://sebastianraschka.com/faq/docs/softmax_regression.html)
- [Yes you should understand backprop, Andrej Karpathy](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)
- [Train with mixed-precision, Nvidia](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
- [The Vanishing Gradient Problem](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)
- [Image Kernels](https://setosa.io/ev/image-kernels/)
- [Distill](https://distill.pub)
- [Making Deep Learning go Brrr from First Principles](https://horace.io/brrr_intro.html)
- [Circuits in NNs, OpenAI](https://distill.pub/2020/circuits/zoom-in/)
- [Too much efficiency makes everything worse: overfitting and the strong version of Goodhart's law](https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html)

For more, look into starred repos on my profile ;) 
